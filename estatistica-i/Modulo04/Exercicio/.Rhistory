#Para mais detalhes do modelo
summary(svmfit)
svmfit=svm(y~.,data=dat,kernel="linear",cost=0.1,scale=FALSE)
plot(svmfit, dat)
svmfit$index
#tune é uma forma de fazer o CV do SVM
set.seed(1)
tune.out=tune(svm, y~.,data=dat,kernel="linear",ranges = list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune.out)
#Pegando o melhor modelo
bestmod=tune.out$best.model
summary(bestmod)
#Gerando os dados de teste
xtest=matrix(rnorm(20*2),ncol=2)
ytest=sample(c(-1,1), 20, rep=TRUE)
xtest[ytest==1,]=xtest[ytest==1,]+1
testdat=data.frame(x=xtest, y=as.factor(ytest))
#Prevendo com o melhor modelo CV
ypred = predict(bestmod, testdat)
table(predict = ypred, truth=testdat$y)
#Testando com um custo diferente
svmfit=svm(y~.,data=dat,kernel="linear",cost=0.01,scale=FALSE)
ypred = predict(svmfit, testdat)
table(predict = ypred, truth=testdat$y)
x[y==1,]=x[y==1,]+0.5
plot(x, col=(y+5)/2, pch=19)
dat=data.frame(x=x, y=as.factor(y))
svmfit=svm(y~.,data=dat,kernel="linear",cost=1e5)
summary(svmfit)
#Alterando o custo para 1
svmfit=svm(y~.,data=dat,kernel="linear",cost=1)
summary(svmfit)
#Trabalhando com outros kernel
set.seed(1)
x=matrix(rnorm(200*2), ncol=2)
x[1:100,]=x[1:100,]+2
x[101:150,]=x[101,150,]-2
x[101:150,]=x[101:150,]-2
y=c(rep(1,150),rep(2,50))
dat=data.frame(x=x, y=as.factor(y))
plot(x, col=y)
train=sample(200,100)
svmfit=svm(y~.,data=dat[train,],kernel="radial",gamma=1,cost=1)
plot(svmfit, dat[train,])
summary(svmfit)
#Por conta do erro elevado talvez seja necessario aumentar o custo mesmo correndo o risco de overfitting
1e5
#Por conta do erro elevado talvez seja necessario aumentar o custo mesmo correndo o risco de overfitting
svmfit=svm(y~.,data=dat[train,],kernel="radial",gamma=1,cost=1e5)
plot(svmfit, dat[train,])
#Utilizando o CV para validar também o valor de gamma
set.seed(1)
tune.out=tune(svm, y~.,data=dat[train,],kernel="radial",ranges=list(cost=c(0.1, 1, 10, 100, 1000)),gamma=c(0.5,1,2,3,4))
summary(tune.out)
tune.out=tune(svm, y~.,data=dat[train,],kernel="radial",ranges=list(cost=c(0.1, 1, 10, 100, 1000),gamma=c(0.5,1,2,3,4)))
summary(tune.out)
#Previsão com os dados de teste
table(true=data[-train,"y"], pred=predict(tune.out$best.model,newdata=dat[-train,]))
#Previsão com os dados de teste
pred=predict(tune.out$best.model,newdata=dat[-train,])
#Previsão com os dados de teste
true=data[-train,"y"]
#Previsão com os dados de teste
table(true=dat[-train,"y"], pred=predict(tune.out$best.model,newdata=dat[-train,]))
#ROC Curves
library(ROCR)
install.packages("ROCR")
#ROC Curves
library(ROCR)
rocplot=function(pred, truth, ...){
predob=prediction(pred, truth)
perf=performance(predob, "tpr", "fpr")
plot(perf,...)
}
svmfit.opt=svm(y~.,data=dat[train,],kernel="radial",gamma=2,cost=1,decision.values=T)
library(e1071)
# Criando dados fictício para a analise
set.seed(1)
x=matrix(rnorm(20*2), ncol=2)
y=c(rep(-1,10), rep(1,10))
x[y==1,]=x[y==1,]+1
plot(x, col=(3-y))
dat = data.frame(x=x, y=as.factor(y))
#kernel linear é usado para dados lineares. cost qto maior mas overfitting e menor mais underfitting
#scale=FALSE faz com que a função não escale variaveis com media zero ou desvio padrão 1
svmfit=svm(y~.,data=dat,kernel="linear",cost=10,scale=FALSE)
#Gera um gráfico com a separação das classes
plot(svmfit, dat)
#Este dados é as observações que fazem parte 7 vetores de suporte (Numero de observações dentro da margem)
svmfit$index
#Para mais detalhes do modelo
summary(svmfit)
svmfit=svm(y~.,data=dat,kernel="linear",cost=0.1,scale=FALSE)
plot(svmfit, dat)
# Com a nova parametrização com o custo menorm a quantidade de vetores aumentou pois a margem esta maior
svmfit$index
#tune é uma forma de fazer o CV do SVM
set.seed(1)
tune.out=tune(svm, y~.,data=dat,kernel="linear",ranges = list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
#Aqui é apresentado o resumo com os melhores parametros
summary(tune.out)
#Pegando o melhor modelo
bestmod=tune.out$best.model
summary(bestmod)
#Gerando os dados de teste
xtest=matrix(rnorm(20*2),ncol=2)
ytest=sample(c(-1,1), 20, rep=TRUE)
xtest[ytest==1,]=xtest[ytest==1,]+1
testdat=data.frame(x=xtest, y=as.factor(ytest))
#Prevendo com o melhor modelo CV
ypred = predict(bestmod, testdat)
table(predict = ypred, truth=testdat$y)
#Testando com um custo diferente
svmfit=svm(y~.,data=dat,kernel="linear",cost=0.01,scale=FALSE)
ypred = predict(svmfit, testdat)
table(predict = ypred, truth=testdat$y)
#Agora com dados com uma separação claramente linear
x[y==1,]=x[y==1,]+0.5
plot(x, col=(y+5)/2, pch=19)
dat=data.frame(x=x, y=as.factor(y))
svmfit=svm(y~.,data=dat,kernel="linear",cost=1e5)
#Gerou uma margem com apenas 3 vetores de suporte
summary(svmfit)
#Alterando o custo para 1
svmfit=svm(y~.,data=dat,kernel="linear",cost=1)
# Este modelo uso 7 vetores e provavelmente terá um desempenho melhor do que o custo de 1e5
summary(svmfit)
#Trabalhando com outros kernel
set.seed(1)
x=matrix(rnorm(200*2), ncol=2)
x[1:100,]=x[1:100,]+2
x[101:150,]=x[101:150,]-2
y=c(rep(1,150),rep(2,50))
dat=data.frame(x=x, y=as.factor(y))
#Analisando o gráfico vemos que o um determinada classe sem encontra no centro
plot(x, col=y)
#Usando o kernel radial
train=sample(200,100)
svmfit=svm(y~.,data=dat[train,],kernel="radial",gamma=1,cost=1)
plot(svmfit, dat[train,])
summary(svmfit)
#Por conta do erro elevado talvez seja necessario aumentar o custo mesmo correndo o risco de overfitting
svmfit=svm(y~.,data=dat[train,],kernel="radial",gamma=1,cost=1e5)
plot(svmfit, dat[train,])
#Utilizando o CV para validar também o valor de gamma
set.seed(1)
tune.out=tune(svm, y~.,data=dat[train,],kernel="radial",ranges=list(cost=c(0.1, 1, 10, 100, 1000),gamma=c(0.5,1,2,3,4)))
summary(tune.out)
#Previsão com os dados de teste
table(true=dat[-train,"y"], pred=predict(tune.out$best.model,newdata=dat[-train,]))
#ROC Curves
library(ROCR)
#Criar um função para gerar o grafico ROC comparando o valor predito com o valor real
rocplot=function(pred, truth, ...){
predob=prediction(pred, truth)
perf=performance(predob, "tpr", "fpr")
plot(perf,...)
}
svmfit.opt=svm(y~.,data=dat[train,],kernel="radial",gamma=2,cost=1,decision.values=T)
fitted=attributes(predict(svmfit.opt,dat[train,],decision.values=TRUE))$decision.values
par(mfrow=c(1,2))
rocplot(fitted,dat[train,"y"],main="Training Data")
# Agora configurando um modelo mais flexivel, com um gamma maior
svmfit.flex=svm(y~.,data=dat[train,],kernel="radial",gamma=50,cost=1,decision.values=T)
fitted=attributes(predict(svmfit.opt,dat[train,],decision.values=TRUE))$decision.values
rocplot(fitted,dat[train,"y"],main="Training Data",add=T,col="red")
svmfit.opt=svm(y~.,data=dat[train,],kernel="radial",gamma=2,cost=1,decision.values=T)
# a relação entre o valor ajustado e a previsão de classe para uma dada observação é simples:
# se o valor ajustado exceder zero, a observação é atribuída a uma classe e se for menor que zero do que é atribuída à outra.
# Para obter os valores ajustados para um determinado ajuste do modelo SVM, usamos decision.values=TRUE
fitted=attributes(predict(svmfit.opt,dat[train,],decision.values=TRUE))$decision.values
par(mfrow=c(1,2))
rocplot(fitted,dat[train,"y"],main="Training Data")
# Agora configurando um modelo mais flexivel, com um gamma maior
svmfit.flex=svm(y~.,data=dat[train,],kernel="radial",gamma=50,cost=1,decision.values=T)
fitted=attributes(predict(svmfit.flex,dat[train,],decision.values=TRUE))$decision.values
rocplot(fitted,dat[train,"y"],main="Training Data",add=T,col="red")
# Criando o gráfico com os dados de teste
fitted=attributes(predict(svmfit.opt,dat[-train,],decision.values=TRUE))$decision.values
rocplot(fitted,dat[train,"y"],main="Test Data")
fitted=attributes(predict(svmfit.flex,dat[-train,],decision.values=TRUE))$decision.values
rocplot(fitted,dat[train,"y"],main="Test Data",add=T,col="red")
#SVM com multiplas classes
set.seed(1)
# Utilizando a abordagem 1 vs 1
x=rbind(x, matrix(rnorm(50*2), ncol=2))
y=c(y, rep(0,50))
x[y==0,2]=x[y==0,2]+2
dat=data.frame(x=x, y=as.factor(y))
par(mfrow=c(1,1))
plot(x, col=(y+1))
svmfit=svm(y~.,data=dat,kernel="radial",gamma=1,cost=10)
plot(svmfit, dat)
#Utilizando o dataset khan
library(ISLR)
names(Khan)
dim(Khan$xtrain)
dim(Khan$xtest)
legth(Khan$ytrain)
legth(Khan$ytest)
length(Khan$ytrain)
length(Khan$ytest)
#Treinando o modelo para prever qual é o tipo de cancer
dat=data.frame(x=Khan$xtrain, y=as.factor(Khan$ytrain))
out=svm(y,data=dat,kernel="linear",cost=10)
summary(out)
#Treinando o modelo para prever qual é o tipo de cancer
dat=data.frame(x=Khan$xtrain, y=as.factor(Khan$ytrain))
#Será utilizando um modelo linear pois o dataset possui muitos atributos e um kernel mais flexível seria desnecessário.
out=svm(y,data=dat,kernel="linear",cost=10)
summary(out)
#Treinando o modelo para prever qual é o tipo de cancer
dat=data.frame(x=Khan$xtrain, y=as.factor(Khan$ytrain))
#Será utilizando um modelo linear pois o dataset possui muitos atributos e um kernel mais flexível seria desnecessário.
out=svm(y~.,data=dat,kernel="linear",cost=10)
summary(out)
table(out$fitted, dat$y)
dat.te=data.frame(x=Khan$xtest, y=as.factor(Khan$ytest))
pred.te=predict(out, newdata=dat.te)
table(pred.te, dat.te$y)
states=row.names(USArrests)
states
names(USArrests)
table(USArrests)
USArrests
#Verificando as diferentes escalas
apply(USArrests, 2, mean)
#Agora a variancia
apply(USArrests, 2, var)
# Quando os dados possui uma diferença significante na escala é importante normalizar, deixando o valor da media proxima de zero e o desvio padrão de 1
pr.out = prcomp(USArrests, scale=TRUE)
names(pr.out)
pr.out
pr.out$center
pr.out$rotation
pr.out$x
biplot(pr.out, scale=0)
biplot(pr.out, scale=0)
pr.out$rotation=-pr.out$rotation
pr.out$x=-pr.out$x
biplot(pr.out, scale=0)
pr.var=pr.out$sdev^2
pr.var
# Calculando a representatividade de cada componentes principais
pve=pr.var/sum(pr.var)
pve
#PC1 => 62% e PC2 24% => esta duas colunas representam
62 + 24
plot(pve, xlab="Principal Component", ylab="Proportion of Variance Explained", ylim=c(0,1), type='b')
# Gráfico os dados acumulados
plot(cumsum(pve), xlab="Principal Component", ylab="Proportion of Variance Explained", ylim=c(0,1), type='b')
#A função cumsum é utilizada para acumulo de valores, segue abaixo um exemplo
a=c(1,2,8,-3)
cumsum(a)
## Clustering
#Kmeans
set.seed(2)
x=matrix(rnrom(50*2), ncol=2)
x=matrix(rnorm(50*2), ncol=2)
x[1:25, 2] = x[1:25,2] - 4
x[1:25, 1] = x[1:25,1] + 3
x[1:25, 2] = x[1:25,2] - 4
x=matrix(rnorm(50*2), ncol=2)
x[1:25, 1] = x[1:25,1] + 3
x[1:25, 2] = x[1:25,2] - 4
km.out=kmeans(x, 2, nstart=20)
km.out$cluster
# Processando a clusterização
km.out=kmeans(x, 2, nstart=20)
km.out$cluster
## Clustering
#Kmeans
set.seed(2)
# Gerando dados fake
x=matrix(rnorm(50*2), ncol=2)
x[1:25, 1] = x[1:25,1] + 3
x[1:25, 2] = x[1:25,2] - 4
# Processando a clusterização
km.out=kmeans(x, 2, nstart=20)
km.out$cluster
# Gráfico com o resultado
plot(x, col=(km.out$cluster+1), main="K-Means Clustering Results with K=2", ylab="", pch=20, cex=2)
# Sabemos que são 2 dois cluster pois são dados fakes mas no mundo real deveria haver algumas tentativas com valores diferentes de K.
set.seed(4)
km.out=kmeans(x, 3, nstart=20)
km.out
# Comparando a performance com diferentes nstart
set.seed(3)
km.out=kmeans(x,3,nstart=1)
km.out$tot.withinss
km.out=kmeans(x,3,nstart=20)
km.out$tot.withinss
km.out=kmeans(x,3,nstart=1)
km.out$tot.withinss
km.out=kmeans(x,3,nstart=20)
km.out$tot.withinss
# Comparando a performance com diferentes nstart
set.seed(3)
km.out=kmeans(x,3,nstart=1)
km.out$tot.withinss
km.out=kmeans(x,3,nstart=20)
km.out$tot.withinss
km.out=kmeans(x,3,nstart=1)
km.out$tot.withinss
km.out=kmeans(x,3,nstart=1)
km.out$tot.withinss
km.out=kmeans(x,3,nstart=20)
km.out$tot.withinss
# O primeiro exemplo será o de PCA com dados de crimes no USA
states=row.names(USArrests)
#Nome dos 50 estados dos EUA
states
#As colunas são os tipo de crimes cometidos
names(USArrests)
#Verificando as diferentes escalas
apply(USArrests, 2, mean)
#Agora a variancia
apply(USArrests, 2, var)
# Quando os dados possui uma diferença significante na escala é importante normalizar, deixando o valor da media proxima de zero e o desvio padrão de 1
pr.out = prcomp(USArrests, scale=TRUE)
# Nome dos atributos
names(pr.out)
# Apresenta o valor da media para normalizar
pr.out$center
# Apresenta o valor do desvio padrão para normalizar
pr.out$scale
# Contem todos os valores de PCA calculados por Colunas
pr.out$rotation
# Contem todos os valores de PCA calculados por Linhas
pr.out$x
# Grafico com as proporções
biplot(pr.out, scale=0)
# Invertendo o gráfico
pr.out$rotation=-pr.out$rotation
pr.out$x=-pr.out$x
biplot(pr.out, scale=0)
# Desvio padrão dos componentes principais
pr.out$sdev
# Variancia dos componentes principais
pr.var=pr.out$sdev^2
pr.var
# Calculando a representatividade de cada componentes principais
pve=pr.var/sum(pr.var)
pve
# Mostrando o resultado no gráfico
plot(pve, xlab="Principal Component", ylab="Proportion of Variance Explained", ylim=c(0,1), type='b')
# Gráfico os dados acumulados
plot(cumsum(pve), xlab="Principal Component", ylab="Proportion of Variance Explained", ylim=c(0,1), type='b')
#A função cumsum é utilizada para acumulo de valores, segue abaixo um exemplo
a=c(1,2,8,-3)
cumsum(a)
## Clustering
#Kmeans
set.seed(2)
# Gerando dados fake
x=matrix(rnorm(50*2), ncol=2)
x[1:25, 1] = x[1:25,1] + 3
x[1:25, 2] = x[1:25,2] - 4
# Processando a clusterização
km.out=kmeans(x, 2, nstart=20)
km.out$cluster
# Gráfico com o resultado
plot(x, col=(km.out$cluster+1), main="K-Means Clustering Results with K=2", ylab="", pch=20, cex=2)
# Sabemos que são 2 dois cluster pois são dados fakes mas no mundo real deveria haver algumas tentativas com valores diferentes de K.
set.seed(4)
km.out=kmeans(x, 3, nstart=20)
km.out
# Comparando a performance com diferentes nstart
set.seed(3)
km.out=kmeans(x,3,nstart=1)
km.out$tot.withinss
km.out=kmeans(x,3,nstart=20)
km.out$tot.withinss
# O objetivo é minimizar esse valor.
#Hierarchical Clustering
hc.comple=hclust(dist(x), method = "complete")
dist(x)
hc.comple=hclust(dist(x), method = "averege")
hc.comple=hclust(dist(x), method = "average")
par(mfrow=c(1,3))
# Metodo: complete
# É a dissimilaridade máxima intercluster. Calcule todas as dissimilaridades de pares entre as observações no cluster A e as observações no cluster B e registre a maior dessas dissimilaridades.
hc.comple=hclust(dist(x), method = "complete")
# Metodo: average
#É a dissimilaridade intercluster média. Calcule todas as dissimilaridades de pares entre as observações no cluster A e as observações no cluster B e registre a média dessas dissimilaridades.
hc.average=hclust(dist(x), method = "average")
# Metodo: single
#É a dissimilaridade intercluster mínima. Calcule todas as dissimilaridades de pares entre as observações no cluster A e as observações no cluster B e registre a menor dessas dissimilaridades. A ligação única pode resultar em clusters estendidos e posteriores, nos quais as observações únicas são fundidas uma de cada vez.
hc.single=hclust(dist(x), method = "single")
par(mfrow=c(1,3))
# Metodo: complete
# É a dissimilaridade máxima intercluster. Calcule todas as dissimilaridades de pares entre as observações no cluster A e as observações no cluster B e registre a maior dessas dissimilaridades.
hc.complete=hclust(dist(x), method = "complete")
par(mfrow=c(1,3))
plot(hc.complete, main="Complete Linkage", xlab="", sub="", cex=.9)
plot(hc.average, main="Average Linkage", xlab="", sub="", cex=.9)
plot(hc.single, main="Single Linkage", xlab="", sub="", cex=.9)
# Obtendo os labels de cada grupo
cutree(hc.complete, 2)
cutree(hc.average, 2)
cutree(hc.single, 2)
# O unico que não separou corretamente para esse caso foi o single, mas se aumentar o grupos ele faz uma melhor separação
cutree(hc.single, 4)
# Aplicando normalização nos dados
xsc=scale(x)
plot(hclust(dist(xsc), method="complete"), main="Hierarchical Clustering with Scaled Features")
x=matrix(rnorm(30*3), ncol=3)
dd=as.dist(1-cor(t(x)))
plot(hclust(dd, method="complete"), main="Complete Linkage with Correlation - Based Distance", xlab="", sub="")
# Dados NCI60
library(ISLR)
nci.labs=NCI60$labs
nci.data=NCI60$data
View(nci.data)
# nci.labs é os tipos de cancer se só será utilizado para comparar a performance das tecnicas de apredizado não supervisionado
dim(nci.data)
# Visualizando os dados dos tipos de cancer
nci.labs[1:4]
table(nci.labs)
# PCA
pr.out=prcomp(nci.data, scale=TRUE)
# Criando 64 cores para ser utilizado na diferenciação de cada grupo
Cols=function(vec){
cols=rainbow(length(unique(vec)))
return(cols[as.numeric(as.factor(vec))])
}
par(mfrow=c(1,2))
plot(pr.out$x[,1:2], col=Cols(nci.labs), pch=19, xlab="Z1", yylab = "Z2")
plot(pr.out$x[,c(1,3)], col=Cols(nci.labs), pch=19, xlab="Z1", yylab = "Z3")
# Analisando os indicadores do PCA
summary(pr.out)
# Analisando graficamente
plt(pr.out)
# Analisando graficamente
plot(pr.out)
# Calculando o PVE
pve=100*pr.out$sdev^2/sum(pr.out$sdev^2)
par(mfrow=c(1,2))
plot(pve, type="o", ylab="PVE", xlab="Principal Component", col="blue")
plot(cumsum(pve), type="o", ylab="Cumulative PVE", xlab="Principal Component", col="brown3")
# Normalização dos dados
sd.data=scale(nci.data)
# Processar os dados com os diferentes tipos
par(mfrow=c(1,3))
data.dist=dist(sd.data)
plot(hclust(data.dist), labels=nci.labs, main="Complete Linkage", xlab="", sub="", ylab="")
plot(hclust(data.dist, method="average"), labels=nci.labs, main="Complete Linkage", xlab="", sub="", ylab="")
plot(hclust(data.dist, method="average"), labels=nci.labs, main="Average Linkage", xlab="", sub="", ylab="")
plot(hclust(data.dist), labels=nci.labs, main="Complete Linkage", xlab="", sub="", ylab="")
plot(hclust(data.dist, method="average"), labels=nci.labs, main="Average Linkage", xlab="", sub="", ylab="")
plot(hclust(data.dist, method="single"), labels=nci.labs, main="Average Linkage", xlab="", sub="", ylab="")
# Fazendo um corte por agrupamento
hc.out=hclust(dist(sd.data))
hc.clusters=cutree(hc.out, 4)
table(hc.clusters,nci.labs)
par(mfrow=c(1,1))
plot(hc.out, labels=nci.labs)
abline(h=139, col="red")
hc.out
# Comparando o resultado do Hierarchical clustering and K-means
set.seed(2)
km.out=kmeans(sd.data, 4, nstart=20)
km.clusters=km.out$cluster
table(km.clusters, hc.clusters)
# Comparando o resultado do Hierarchical clustering and PCA
hc.out=hclust(dist(pr.out$x[,1:5]))
plot(hc.out, labels=nci.labs,main="Hier. Clustt on First Five Score Vectores")
table(cutree(hc.out,4), nci.labs)
setwd("~/git/curso_dsa/estatistica-i/Modulo04/Exercicio")
mydata <- read.csv("dataset.csv")
# Qual o tempo médio de atendimento
mean(mydata$tempo_telefone)
# Qual a medianado tempo de atendimento?
median(mydata$tempo_telefone)
# Qual o desvio padrão e variância do tempo de atendimento?
desvios <- (mydata$tempo_telefone - mean(mydata$tempo_telefone))
desvios_padrao <- sqrt(sum(desvios)^2/(length(mydata)-1))
desvios_padrao
sd(mydata$tempo_telefone)
variancia <- sum(desvios)^2/(length(mydata)-1)
variancia
Q1 <- quantile(mydata$tempo_telefone, probs = 0.25)
Q1
Q2 <- quantile(mydata$tempo_telefone, probs = 0.50)
Q2
Q3 <- quantile(mydata$tempo_telefone, probs = 0.75)
Q3
perc_32 <- quantile(mydata$tempo_telefone, probs = 0.32)
perc_32
perc_57 <- quantile(mydata$tempo_telefone, probs = 0.57)
perc_57
perc_98 <- quantile(mydata$tempo_telefone, probs = 0.98)
perc_98
# Qual o desvio padrão e variância do tempo de atendimento?
desvios_padrao <- sd(mydata$tempo_telefone)
desvios_padrao
variancia <- var(mydata$tempo_telefone)
variancia
Q1 <- quantile(mydata$tempo_telefone, probs = 0.25)
Q1
Q2 <- quantile(mydata$tempo_telefone, probs = 0.50)
Q2
Q3 <- quantile(mydata$tempo_telefone, probs = 0.75)
Q3
perc_32 <- quantile(mydata$tempo_telefone, probs = 0.32)
perc_32
perc_57 <- quantile(mydata$tempo_telefone, probs = 0.57)
perc_57
perc_98 <- quantile(mydata$tempo_telefone, probs = 0.98)
perc_98
boxplot(mydata$tempo_telefone)
boxplot(mydata$tempo_telefone, horizontal = TRUE)
