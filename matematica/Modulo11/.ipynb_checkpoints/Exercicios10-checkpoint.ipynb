{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Academy</font>\n",
    "# <font color='blue'>Matemática Para Machine Learning</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lista de Exercícios  - Capítulo 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objetivo desta lista de exercícios é você praticar os principais conceitos estudados neste capítulo, ao mesmo tempo que desenvolve suas habilidades em lógica de programação com a linguagem Python. \n",
    "\n",
    "Caso tenha dúvidas, isso é absolutamente normal e faça um trabalho de pesquisa a fim de relembrar o formato das operações matemáticas.\n",
    "\n",
    "Quando encontrar o formato de uma operação que resolva o exercício proposto, use a linguagem Python para representar esta operação. Em essência, é assim que aplicamos Matemática Para Machine Learning, construindo algoritmos e representando esses algoritmos em linguagem de programação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divirta-se!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Cálculo, mais apropriadamente chamado de análise, é o ramo da Matemática que estuda a taxa de variação de quantidades (que pode ser interpretada como declives de curvas) além do comprimento, a área e o volume dos objetos. \n",
    "\n",
    "A palavra Cálculo vem do latim que significa \"pequena pedra\" (small stone), porque é como entender algo olhando pequenos pedaços.\n",
    "\n",
    "Cálculo é um campo intrínseco de Matemática e a essência em muitos algoritmos de aprendizado de máquina. Sua compreensão é fundamental para quem pretende trabalhar com Data Science.\n",
    "\n",
    "O Cálculo é dividido em Cálculo Diferencial e Integral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"images/image01.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De forma bem simples:\n",
    "\n",
    "* Cálculo Diferencial - corta \"algo\" em pequenos pedaços para descobrir como \"algo\" muda.\n",
    "\n",
    "* Cálculo Integral - junta (integra) os pequenos pedaços para descobrir o quanto existe como um todo.\n",
    "\n",
    "Recomendamos que você assista aos vídeos sobre Cálculo do canal 3blue1brown, que ensina alguns pilares importantes do Cálculo de forma bem visual:\n",
    "\n",
    "https://www.3blue1brown.com/\n",
    "\n",
    "Há ainda um ótimo material com todas as fórmulas e exemplos sobre Limites, Diferencial e Integral:\n",
    "\n",
    "https://www.mathsisfun.com/calculus/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculando Derivada e Integral com SymPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://peerj.com/articles/cs-103.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 1 - Calcule a derivada de \"x elevado ao quadrado\".\n",
    "\n",
    "Dica: Para obter derivadas, use a função diff com SymPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solução\n",
    "from sympy import *\n",
    "x, y, z = symbols('x y z')\n",
    "init_printing(use_unicode=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 2 - Calcule a terceira derivada de \"x elevado a quarta potência\".\n",
    "\n",
    "Dica: A função diff pode calcular várias derivadas de uma só vez. Para obter várias derivadas, passe a variável quantas vezes quiser diferenciar ou passe um número após a variável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solução\n",
    "from sympy import *\n",
    "x, y, z = symbols('x y z')\n",
    "init_printing(use_unicode=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para criar uma derivada não avaliada (ou seja, apenas sua representação em fórmula), use a classe Derivative(). Tem a mesma sintaxe do diff. Para isso, considere a expressão: (expr, x, y, y, z, 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import *\n",
    "x, y, z = symbols('x y z')\n",
    "init_printing(use_unicode=True)\n",
    "\n",
    "expr = exp(x*y*z)\n",
    "deriv = Derivative(expr, x, y, y, z, 4)\n",
    "deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 3 - Execute a derivada não avaliada que você definiu no item anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solução\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 4 - Calcule a integral do coseno de uma variável x.\n",
    "\n",
    "Para calcular uma integral, use a função integrate(). Existem dois tipos de integrais, definidos e indefinidos. Para calcular uma integral indefinida, isto é, uma antiderivada, ou primitiva, basta passar a variável após a expressão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solução\n",
    "from sympy import *\n",
    "x, y, z = symbols('x y z')\n",
    "init_printing(use_unicode=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para representar a fórmula da Interal, usamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = Integral(log(x)**2, x)\n",
    "print(expr)\n",
    "expr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolvendo Equações Diferenciais com SymPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 5 - Para resolver equações diferenciais com SymPy, use a função dsolve(). Primeiro, crie uma função indefinida passando cls = Function para a função de símbolos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esses objetos não avaliados são úteis para atrasar a avaliação da derivada ou para fins de impressão. Eles também são usados quando o SymPy não sabe como calcular a derivada de uma expressão (por exemplo, se ela contém uma função indefinida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solução\n",
    "\n",
    "# f e g são funções indefinidas. Podemos chamar f(x) e isso representará uma função desconhecida.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivadas de f(x) não são avaliadas.\n",
    "f(x).diff(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para representar a equação diferencial f″(x) − 2f′(x) + f(x) = sin(x), usaríamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffeq = Eq(f(x).diff(x, x) - 2*f(x).diff(x) + f(x), sin(x))\n",
    "diffeq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para resolver a equação, usamos dsolve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsolve(diffeq, f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dsolve retorna uma instância da Eq. Isso ocorre porque, em geral, soluções para equações diferenciais não podem ser resolvidas explicitamente para a função. As constantes arbitrárias nas soluções de dsolve são símbolos da forma C1, C2, C3 e assim por diante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsolve(f(x).diff(x)*(1 - sin(f(x))), f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente Descendente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://deeplearningbook.com.br/aprendizado-com-a-descida-do-gradiente/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um gradiente mede o quanto a saída de uma função muda se você alterar um pouco as entradas. \n",
    "\n",
    "A maioria das tarefas em Machine Learning são na verdade problemas de otimização e um dos algoritmos mais usados para isso é o Algoritmo de Descida do Gradiente. Para um iniciante, o nome Algoritmo de Descida do Gradiente pode parecer intimidante, mas espero que depois de ler o que está logo abaixo, isso deixe de ser um mistério para você.\n",
    "\n",
    "A Descida do Gradiente é uma ferramenta padrão para otimizar funções complexas iterativamente dentro de um programa de computador. Seu objetivo é: dada alguma função arbitrária, encontrar um mínimo. Para alguns pequenos subconjuntos de funções – aqueles que são convexos – há apenas um único mínimo que também acontece de ser global. Para as funções mais realistas, pode haver muitos mínimos, então a maioria dos mínimos são locais. Certifique-se de que a otimização encontre o “melhor” mínimo e não fique preso em mínimos sub-otimistas (um problema comum durante o treinamento do algoritmo).\n",
    "\n",
    "Ou seja, a Descida do Gradiente é um algoritmo de otimização usado para encontrar os valores de parâmetros (coeficientes ou se preferir w e b – weight e bias) de uma função que minimizam uma função de custo. A Descida do Gradiente é melhor usada quando os parâmetros não podem ser calculados analiticamente (por exemplo, usando álgebra linear) e devem ser pesquisados por um algoritmo de otimização.\n",
    "\n",
    "Suponha que você tenha uma bola e uma tigela. Não importa onde você deslize a bola na tigela, ela acabará caindo no fundo da tigela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"images/image02.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como você pode ver, esta bola segue um caminho que termina no fundo da tigela. As linhas vermelhas são o gradiente da tigela e a linha azul é o caminho da bola, e como o caminho da inclinação da bola está diminuindo, ela é chamada de gradiente de descida (gradiente descendente).\n",
    "\n",
    "Em nosso modelo de aprendizado de máquina, nosso objetivo é reduzir o custo. A função custo é usada para monitorar o erro nas previsões de um modelo de Machine Learning. Portanto, minimizar isso significa basicamente obter o menor valor de erro possível ou aumentar a precisão do modelo. Em suma, aumentamos a precisão iterando sobre um conjunto de dados de treinamento ao ajustar os parâmetros (os pesos e vieses) do nosso modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos considerar o seguinte exemplo. Imagine que uma universidade queira prever a profissão dos alunos com base em suas notas em uma série de disciplinas. Coletamos então dados passados de notas e as profissões dos alunos. Nosso objetivo é prever a profissão dos alunos considerando as notas em algumas disciplinas. Observe a tabela abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"images/image03.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste conjunto de dados, temos dados de Adão e Eva. Usando como referência os dados de Adão e Eva, temos que prever a profissão de João.\n",
    "\n",
    "Agora pense nas notas como um gradiente e profissão como o alvo inferior na \"tigela\". Você precisa otimizar seu modelo para que o resultado previsto na parte inferior seja preciso. Usando os dados de Adão e Eva, usaremos gradiente descendente e ajustaremos nosso modelo de tal forma que, se inserirmos as notas de Adão, ele deverá prever o resultado de Médico na parte inferior do gradiente e o mesmo para Eva que é Engenheira. Este é o nosso modelo treinado. Agora, se dermos novas notas ao nosso modelo (as notas do João), podemos facilmente prever sua profissão.\n",
    "\n",
    "Em teoria, isso é gradiente descendente, mas para calcular e modelar o gradiente descendente usamos as ferramentas oferecidas pelo Cálculo e agora podemos ver a importância do Cálculo na aprendizagem de máquina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"images/image04.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro vamos começar pelo tópico com o qual você já deve estar bem familiarizado: Álgebra Linear. Vamos usar a Álgebra Linear e sua fórmula para o nosso modelo.\n",
    "\n",
    "A fórmula básica que podemos usar neste modelo é:\n",
    "\n",
    "y = m * x + b\n",
    "\n",
    "Onde,\n",
    "\n",
    "* y = valor a ser previsto \n",
    "* m = declive da reta\n",
    "* x = dado de entrada (variável preditora)\n",
    "* b = intercepção no eixo y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"images/image05.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma abordagem padrão para resolver esse tipo de problema é definir uma função de erro (também chamada de função de custo) que mede o quão “boa” é uma determinada linha. Esta função terá um par (m, b) e retornará um valor de erro baseado em quão bem a linha se ajusta aos nossos dados. Para calcular esse erro para uma determinada linha, iteramos por cada ponto (x, y) em nosso conjunto de dados e somamos as distâncias quadradas entre o valor y de cada ponto e o valor y da linha candidata (calculado em mx + b). É convencional elevar ao quadrado essa distância para garantir que ela seja positiva e tornar nossa função de erro diferenciável (Método dos Mínimos Quadrados)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"images/image06.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linhas que se ajustam melhor aos nossos dados (onde melhor é definido pela nossa função de erro) resultarão em valores de erro menores. Se minimizarmos essa função, obteremos a melhor linha para nossos dados. Como nossa função de erro consiste em dois parâmetros (m e b), podemos visualizá-lo como uma superfície bidimensional. Isto é o que parece para o nosso conjunto de dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"images/image07.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada ponto neste espaço bidimensional (gráfico da esquerda) representa uma linha. A altura da função em cada ponto é o valor de erro dessa linha. Você pode ver que algumas linhas geram valores de erro menores do que outras (isto é, ajustam melhor nossos dados). Quando executamos a busca por gradiente descendente, começaremos de algum local nessa superfície e desceremos para encontrar a linha com o menor erro.\n",
    "\n",
    "Pelo que viu nas aulas de Cálculo até aqui, você sabe que para calcular a inclinação, usamos a Cálculo Diferencial e calculamos a derivada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"images/image08.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para executar o gradiente descendente nesta função de erro, primeiro precisamos calcular seu gradiente. O gradiente agirá como uma bússola e sempre nos apontará para baixo. Para calculá-lo, precisaremos diferenciar nossa função de erro. Como nossa função é definida por dois parâmetros (m e b), precisaremos calcular uma derivada parcial para cada um, da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"images/image09.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora temos todas as ferramentas necessárias para executar a descida do gradiente. Podemos inicializar nossa busca para iniciar em qualquer par de valores m e b (ou seja, qualquer linha) e deixar que o algoritmo de descida do gradiente desacelere em nossa função de erro em direção à melhor linha. Cada iteração atualizará m e b para uma linha que produz um erro ligeiramente menor que a iteração anterior. A direção a ser movida para cada iteração é calculada usando as duas derivadas parciais de cima.\n",
    "\n",
    "A variável Taxa de Aprendizado (Learning Rate) controla o tamanho de um passo que fazemos durante cada iteração. Se dermos um passo muito grande, podemos ultrapassar o mínimo. No entanto, se tomarmos pequenos passos, serão necessárias muitas iterações para chegar ao mínimo e consequentemente mais tempo de treinamento do modelo.\n",
    "\n",
    "É isso! É assim que treinamos um algoritmo de Machine Learning com base na descida do gradiente (diversos algoritmos usam esta técnica).\n",
    "\n",
    "Mais alguns conceitos importantes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convexidade - Em nosso problema de regressão linear, havia apenas um mínimo. Nossa superfície de erro era convexa. Independentemente de onde começamos, chegaríamos ao mínimo absoluto. Mas em geral, isso não é o caso. É possível ter um problema com os mínimos locais em que uma pesquisa de gradiente pode ficar presa. Há várias abordagens para atenuar isso (por exemplo, pesquisa de gradiente estocástico)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"images/image10.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convergência - Não falamos sobre como determinar quando a pesquisa encontra uma solução. Isso geralmente é feito procurando pequenas alterações iteração a iteração (por exemplo, onde o gradiente é próximo de zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"images/image11.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Como Calcular as Derivadas Parciais\n",
    "\n",
    "Como tomamos as derivadas de uma função como a seguinte?\n",
    "\n",
    "$$f(x,y) = x^2 + y^2$$\n",
    "\n",
    "Podemos tomar uma derivada das mudanças na função em relação a x ou y. Chamamos essas derivadas com relação a uma variável de derivada parcial. Vamos tentar isso tomando a derivada de $f(x, y)$ em relação a *** x ***. Escrevemos essa derivada parcial da seguinte maneira.\n",
    "\n",
    "$$\\frac{\\partial f(x,y)}{\\partial x} = \\frac{\\partial (x^2 + y^2)}{\\partial x}$$\n",
    "\n",
    "Assim como derivadas comuns nos dão uma maneira de calcular a taxa de mudança de uma função, derivadas parciais nos dão uma maneira de calcular a taxa de mudança de uma função de muitas variáveis em relação a uma dessas variáveis.\n",
    "\n",
    "Como $f(x, y)$ é a soma de várias funções mais simples, precisamos tomar a derivada parcial de cada uma delas e somar o resultado. As duas primeiras partes são fáceis.\n",
    "\n",
    "$$\\frac{\\partial x^2}{\\partial x} = 2x$$\n",
    "\n",
    "Observe que estamos seguindo as regras usuais de diferenciação para qualquer função de *** x *** aqui.\n",
    "\n",
    "Agora precisamos pegar a derivada parcial da última parte de $f(x, y)$, que não depende de *** x ***. Nestes termos, obtemos o seguinte.\n",
    "\n",
    "$$\\frac{\\partial y^2}{\\partial x} = 0$$\n",
    "\n",
    "Agora podemos adicionar as partes para obter a derivada parcial completa de $f(x, y)$.\n",
    "\n",
    "$$\\frac{\\partial f(x,y)}{\\partial x} = 2x + 0 = 2x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nós também podemos pegar a derivada parcial de $f(x, y)$ em relação a *** y ***. O processo prossegue da seguinte maneira.\n",
    "\n",
    "$$\\frac{\\partial f(x,y)}{\\partial y} = 0 + 2y = 2y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Computando Um Gradiente\n",
    "\n",
    "Você pode perguntar qual é o ponto de calcular derivadas parciais? Sim, eles são um ótimo truque de Matemática, mas para que servem? Acontece que derivadas parciais são importantes se você quiser encontrar o análogo da inclinação para superfícies multidimensionais. Nós chamamos essa quantidade de ** gradiente **.\n",
    "\n",
    "Lembre-se de que você pode encontrar o mínimo e o máximo de curvas usando derivadas. Da mesma forma, você pode encontrar o mínimo e o máximo de superfícies seguindo o gradiente e encontrando os pontos onde o gradiente é zero em todas as direções."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você já examinou as derivadas parciais da função, $f(x,y) = x^2 + y^2$. Estas derivadas parciais são:\n",
    "\n",
    "$$\\frac{\\partial f(x,y)}{\\partial x} = 2x \\\\\n",
    "\\frac{\\partial f(x,y)}{\\partial y} = 2y$$\n",
    "\n",
    "Neste caso, o gradiente é um vetor bidimensional da mudança da função na direção $x$ e a mudança na função na direção $y$. Este vetor pode ser escrito da seguinte forma:\n",
    "\n",
    "$$grad(f(x,y)) =  \\vec{g(x,y)} = \\begin{bmatrix}\\frac{\\partial f(x,y)}{\\partial x} \\\\ \\frac{\\partial f(x,y)}{\\partial y} \\end{bmatrix} = \\begin{bmatrix}2x \\\\ 2y \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotando o Gradiente\n",
    "\n",
    "Um plot irá ajudá-lo a sentir o significado do gradiente. O código abaixo representa o gradiente da função $f(x,y) = x^2 + y^2$ junto com contornos do valor da função. Execute este código e examine o plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 6 - Analise cuidadosamente o código abaixo e complete o código onde você encontrar a palavra \"TODO\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Cria uma grade uniforme\n",
    "el = np.arange(-5,6)\n",
    "nx, ny = np.meshgrid(el, el, sparse=False, indexing='ij')\n",
    "\n",
    "# Define as listas\n",
    "x_coord = []\n",
    "y_coord = []\n",
    "z = []\n",
    "\n",
    "# Flatten da grade para 1-d e calcular o valor da função z\n",
    "for i in range(11):  \n",
    "    for j in range(11):\n",
    "        x_coord.append(float(-nx[i,j]))\n",
    "        y_coord.append(float(-ny[i,j]))       \n",
    "        z.append(nx[i,j]**2 + ny[i,j]**2)\n",
    "\n",
    "# Realizar aritmética vetorial para obter os gradientes de x e y      \n",
    "x_grad = TODO\n",
    "y_grad = TODO\n",
    "\n",
    "# Plot das setas usando largura para gradiente\n",
    "plt.xlim(-5.5,5.5)\n",
    "plt.ylim(-5.5,5.5)\n",
    "\n",
    "for x, y, xg, yg in zip(list(x_coord), list(y_coord), list(x_grad), list(y_grad)):\n",
    "    if x != 0.0 or y != 0.0: \n",
    "        \n",
    "        # Evite a divisão por zero ao dimensionar a seta\n",
    "        l = math.sqrt(xg**2 + yg**2)/2.0\n",
    "        plt.quiver(x, y, xg, yg, width = l, units = 'dots')\n",
    "\n",
    "# Plot dos contornos da superfície da função\n",
    "z = np.array(z).reshape(11,11)    \n",
    "plt.contour(el, el, z)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Observe as seguintes propriedades deste gráfico.\n",
    "\n",
    "- As setas no ponto de plotagem na direção do gradiente.\n",
    "- A largura das setas é proporcional ao valor do gradiente. A largura das setas e o gradiente ** diminui à medida que a função se aproxima do mínimo **. Se este for o caso em todos os lugares, você pode dizer que uma função é ** convexa **. É sempre muito mais fácil encontrar o mínimo de funções convexas.\n",
    "- A direção ** do gradiente é sempre perpendicular aos contornos **. Esta é uma propriedade importante das funções multivariadas.\n",
    "\n",
    "## Usando o gradiente\n",
    "\n",
    "Então, para que tudo isso? Digamos que você queira encontrar o mínimo da função $f(x,y) = x^2 + y^2$. É fácil ver que o mínimo dessa função é de $x = 0$ e $y = 0$. Mas, e se você não conhecesse essa solução? Então você poderia fazer o seguinte:\n",
    "\n",
    "1. Tome algum palpite inicial.\n",
    "2. Calcule o gradiente.\n",
    "3. Dê um pequeno passo na direção do gradiente.\n",
    "4. Determine se o gradiente está próximo de zero. Em caso afirmativo, pare, pois o gradiente será zero no mínimo.\n",
    "5. Repita as etapas 2, 3 e 4.\n",
    "\n",
    "O algoritmo descrito acima é chamado de ** Algoritmo Gradiente Descendente **. É a base de muitos algoritmos de minimização do mundo real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dica: Para exercícios interativos em Cálculo, acesse: https://brilliant.org/calculus/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fim"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
