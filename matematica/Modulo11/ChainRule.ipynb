{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Academy</font>\n",
    "# <font color='blue'>Matemática Para Machine Learning</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estudo de Caso 6 - Regra da Cadeia em Modelo de Deep Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introdução\n",
    "\n",
    "As derivadas são uma parte crítica do aprendizado de máquina, particularmente as redes neurais profundas (Deep Learning), que são treinadas otimizando uma função de perda (função de custo). Ao fazer a leitura de um paper de aprendizado de máquina ou a documentação de uma biblioteca como PyTorch ou TensorFlow, Cálculo estará por todas as partes. E não estamos falando apenas de Cálculo escalar, mas sim de Cálculo matricial diferencial, especialmente Cálculo multivariado.\n",
    "\n",
    "É possível desenvolver modelos de Machine Learning com um nível mínimo de conhecimento em Cálculo, graças à facilidade embutida nas modernas bibliotecas de aprendizado profundo. Mas, se você realmente quiser entender o que está acontecendo sob o capô dessas bibliotecas e analisar os trabalhos acadêmicos que discutem os mais recentes avanços nas técnicas de treinamento de modelos ou mesmo desenvolver seus próprios modelos a partir do zero, você precisará entender certas partes do campo do Cálculo matricial e multivariado.\n",
    "\n",
    "Este estudo de caso tem como objetivo detalhar o funcionamento de uma das técnicas de Cálculo mais usadas no treinamento de algoritmos de Deep Learning: a Regra da Cadeia. Bons estudos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tudo Começa Pelo Neurônio em Deep Learning\n",
    "\n",
    "A ativação de uma única unidade de computação em uma rede neural é tipicamente calculada usando o produto escalar (da álgebra linear) de um vetor de peso de aresta w com um vetor de entrada x mais um desvio escalar (limiar). A função é chamada de função afinne da unidade e é seguida por uma unidade linear retificada, que filtra valores negativos para zero. Tal unidade computacional é algumas vezes referida como um “neurônio artificial” e se parece com:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"images/neuron.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As redes neurais consistem em muitas dessas unidades, organizadas em várias coleções de neurônios chamadas camadas. A ativação de unidades de uma camada se torna a entrada para as próximas unidades da camada. A ativação da unidade ou unidades na camada final é chamada de saída de rede.\n",
    "\n",
    "Treinar este neurônio significa escolher pesos w e viés b para que possamos obter a saída desejada para todas as N entradas x. Para fazer isso, minimizamos uma função de perda que compara o final da rede com a (saída desejada de x) para todos os vetores x de entrada. Para minimizar a perda, usamos algumas variações na descida do gradiente, como descida de gradiente estocástica simples (SGD), SGD com momentum ou Adam. Todos eles requerem a derivada parcial (o gradiente) com respeito aos parâmetros do modelo w e b. Nosso objetivo é ajustar gradualmente w e b de modo que a função de perda geral continue diminuindo em todas as entradas x. Vamos estudar isso em mais detalhes no próximo capítulo.\n",
    "\n",
    "Se tivermos cuidado, podemos derivar o gradiente diferenciando a versão escalar de uma função de perda comum (erro quadrático médio):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"images/formula1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas isso é apenas um neurônio, e as redes neurais devem treinar os pesos e vieses de todos os neurônios em todas as camadas simultaneamente. Como existem múltiplas entradas e (potencialmente) múltiplas saídas de rede, nós realmente precisamos de regras gerais para a derivada de uma função com relação a um vetor e até regras para a derivada de uma função com valor de vetor em relação a um vetor.\n",
    "\n",
    "Este estudo de caso percorre a derivação de algumas regras importantes para a computação de derivadas parciais em relação a vetores, particularmente aqueles úteis para o treinamento de redes neurais. Esse campo é conhecido como cálculo matricial, e a boa notícia é que precisamos apenas de um pequeno subconjunto desse campo, que apresentamos aqui. Embora haja muito material online sobre cálculo multivariado e álgebra linear, eles são normalmente ensinados como dois cursos de graduação separados, de modo que a maioria dos materiais os trata isoladamente. As páginas que discutem o cálculo da matriz são, na verdade, apenas listas de regras com uma explicação mínima ou são apenas partes da história. Eles também tendem a ser bastante obscuros para todos, exceto para uma estreita audiência de matemáticos, graças ao uso de notação densa e discussão mínima de conceitos fundamentais. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em contraste, vamos rederivar e redescobrir algumas regras fundamentais de cálculo matricial em um esforço para explicá-las. Acontece que o cálculo matricial não é tão difícil assim! Não há dezenas de novas regras para aprender; apenas alguns conceitos-chave. Nossa expectativa é que este breve estudo de caso o ajude rapidamente no mundo do cálculo matricial no que se refere ao treinamento de redes neurais. Estamos assumindo que você já está familiarizado com os conceitos básicos de arquitetura e treinamento de redes neurais. Se você não estiver, faça a leitura do Deep Learning Books ou acesse o capítulo de Redes Neurais do curso gratuito Python Fundamentos Para Análise de Dados (se estiver fazendo nossas demais Formações, acesse o curso de Machine Learning da FCD ou o curso Deep Learning I da FIA). Note que, diferente de muitas outras abordagens acadêmicas, nós sugerimos primeiramente aprender a treinar e usar redes neurais na prática e então estudar a Matemática em seguida. A Matemática será muito mais compreensível com o contexto em vigor. Além disso, não é necessário ser um Matemático e dominar tudo sobre cálculo para se tornar um praticante eficaz de Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regras Derivativas Escalares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"images/regras.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando uma função tem um único parâmetro, f(x), você geralmente verá f' e f'(x) e usará atalhos para:\n",
    "\n",
    "$$\\frac{df}{dx} f(x)$$\n",
    "\n",
    "Recomendamos contra essa notação, uma vez que não deixa clara a variável na qual estamos tomando a derivada em relação a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você pode pensar em $$\\frac{d}{dx}$$ \n",
    "\n",
    "como um operador que mapeia uma função de um parâmetro para outra função. Isso significa que \n",
    "\n",
    "$$\\frac{df}{dx} f(x)$$ mapeia f(x) a sua derivada em relação a x, que é a mesma coisa que $$\\frac{df(x)}{dx}$$ \n",
    "\n",
    "Pensar na derivada como um operador ajuda a simplificar derivações complicadas porque o operador é distributivo e nos permite extrair constantes. Por exemplo, na equação a seguir, podemos extrair a constante 9 e distribuir o operador derivativo entre os elementos dentro dos parênteses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"images/formula2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse procedimento reduziu a derivada $$9(x + x^2)$$ \n",
    "\n",
    "de um pouco de aritmética e as derivadas de x, que são muito mais fáceis de resolver do que a derivada original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introdução ao cálculo vetorial e derivadas parciais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Camadas de redes neurais não são funções únicas de um único parâmetro, f(x). Então, vamos passar para funções de múltiplos parâmetros como f(x,y). Por exemplo, qual é a derivada de xy (isto é, a multiplicação de x e y)? Em outras palavras, como o produto xy muda quando mexemos nas variáveis? Bem, isso depende se estamos mudando x ou y. Calculamos derivadas com relação a uma variável (parâmetro) por vez, dando-nos duas derivadas parciais diferentes para essa função de dois parâmetros (uma para x e um para y). Em vez de usar o operador \n",
    "\n",
    "$$\\frac{d}{dx}$$ \n",
    "\n",
    "o operador de derivada parcial é $$\\frac{\\partial}{\\partial x}$$\n",
    "\n",
    "Então, $$\\frac{\\partial}{\\partial x}xy$$ e $$\\frac{\\partial}{\\partial y}xy$$ \n",
    "\n",
    "são as derivadas parciais de xy; muitas vezes, essas são chamadas apenas de parciais. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A derivada parcial em relação a x é apenas a derivada escalar usual, simplesmente tratando qualquer outra variável na equação como uma constante. Portanto, o gradiente de f(x,y) é simplesmente um vetor de suas derivadas parciais. Os gradientes fazem parte do mundo do cálculo vetorial, que lida com funções que mapeiam n parâmetros escalares para um único escalar. Agora, vamos ficar loucos e considerar derivadas de múltiplas funções simultaneamente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Regra da Cadeia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não podemos calcular derivadas parciais de funções muito complicadas usando apenas as regras básicas de cálculo matricial. Por exemplo, não podemos obter a derivada de expressões aninhadas como sum(w + x) diretamente sem reduzí-la ao seu equivalente escalar. Precisamos ser capazes de combinar nossas regras básicas de vetores usando o que podemos chamar de regra da cadeia de vetores. Infelizmente, há uma série de regras para diferenciação que se enquadram no nome de \"regra da cadeia\", por isso temos que ter cuidado com qual regra da cadeia estamos falando. Vamos descrever aqui a regra da cadeia de variável única, onde queremos a derivada de uma função escalar com relação a um escalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A regra da cadeia é conceitualmente uma estratégia de divisão e conquista (como Quicksort) que divide expressões complicadas em subexpressões cujas derivações são mais fáceis de serem computadas. Seu poder deriva do fato de que podemos processar cada subexpressão simples isoladamente, mas ainda combinar os resultados intermediários para obter o resultado geral correto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A regra da cadeia entra em cena quando precisamos da derivada de uma expressão composta de subexpressões aninhadas. Por exemplo, precisamos da regra da cadeia quando confrontados com expressões como. A expressão mais externa leva a um resultado intermediário, uma subexpressão aninhada que compara x. Especificamente, precisamos da regra da cadeia de variável única, então vamos começar nos aprofundando nisso com mais detalhes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos começar com a solução para a derivada da nossa expressão aninhada\n",
    "\n",
    "$$\\frac{d}{dx}sin(x^2) = 2xcos(x^2)$$ \n",
    "\n",
    "Não é preciso um gênio matemático para reconhecer componentes da solução que cheiram a regras de diferenciação escalar\n",
    "\n",
    "$$\\frac{d}{dx}sin(x^2) = 2xcos(x^2)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As regras de cadeia geralmente são definidas em termos de funções aninhadas, como y = f(g(x)) para regras de cadeia de variável única. (Você também verá a regra da cadeia definida usando a composição da função, que é a mesma coisa.) Algumas fontes gravam a derivada usando a notação abreviada, mas isso esconde o fato de que estamos introduzindo uma variável intermediária:, que veremos em breve. É melhor definir explicitamente a regra da cadeia de variável única, de modo que nunca tomemos a derivada com relação à variável errada. Aqui está a formulação da regra da cadeia de variável única que recomendamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"images/formula3.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para implantar a regra de cadeia de variável única, siga estas etapas:\n",
    "\n",
    "1- Introduzir variáveis intermediárias para subexpressões e subexpressões aninhadas para operadores binários e unários; por exemplo, é binária, e outras funções trigonométricas são geralmente unárias porque existe um único operando. Esta etapa normaliza todas as equações para operadores únicos ou aplicativos de função.\n",
    "\n",
    "2- Compute derivadas das variáveis intermediárias em relação aos seus parâmetros.\n",
    "\n",
    "3- Combine todas as derivadas das variáveis intermediárias, multiplicando-as para obter o resultado geral.\n",
    "Substitua as variáveis intermediárias de volta, se houver alguma referência na equação derivada.\n",
    "\n",
    "4- O terceiro passo coloca a “cadeia” na “regra da cadeia” porque ela une os resultados intermediários. Multiplicar as derivadas intermediárias é o tema comum entre todas as variações da regra da cadeia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referências:\n",
    "\n",
    "Matrix Calculus\n",
    "http://www.ee.ic.ac.uk/hp/staff/dmb/matrix/calculus.html\n",
    "    \n",
    "The Matrix Cookbook\n",
    "https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf\n",
    "\n",
    "Matrix Differentiation\n",
    "https://www.comp.nus.edu.sg/~cs5240/lecture/matrix-differentiation.pdf\n",
    "\n",
    "The Chain Rule of Calculus\n",
    "https://eli.thegreenplace.net/2016/the-chain-rule-of-calculus/\n",
    "\n",
    "Deep Learning Book: http://www.deeplearningbook.com.br/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fim"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
